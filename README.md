# awesome-LLM-papers
经典LLM论文阅读和复现。【常读常新】

# 目录
[Base Model](#1-基础模型)
[Alignment](#2-价值对齐)
[Dataset](#3-数据集)
[Reasoning](#4-推理增强)
[Agent](#5-智能体)


# 1. 基础模型

# 2. 价值对齐

## 2.1 Deep Reinforcement Learning from Human Preferences. 201706. OpenAI. NIPS17.

<details>
    <summary>点击展开/折叠（论文摘要）</summary>
   
## **研究基础**
- 研究背景：传统RL依赖明确奖励函数，现实复杂任务难以量化设计；IRL/模仿学习依赖人类演示，直接人类反馈成本过高
- 研究目标：仅需人类识别目标行为、支持非专家教学、适配复杂任务、节省反馈成本
- 相关工作：偏好型RL已有研究，但局限于简单任务；本文首次将其规模化至深度RL复杂场景
## **核心方法**
- 核心框架：策略与环境交互→人类偏好反馈→奖励函数更新，三过程异步运行
- 策略优化：Atari用A2C、机器人任务用TRPO；奖励归一化，调熵值促进探索
- 偏好获取：1-2秒短视频片段对比，人类判断偏好/平局/无法比较，降低判断难度
- 奖励函数拟合：基于Bradley-Terry模型，交叉熵损失匹配偏好概率；集成预测器+L2正则+dropout防过拟合
- 查询选择：优先选集成预测器分歧最大的片段对，提升反馈信息价值
## **实验设计与结果**
- 实验设置：测试8个MuJoCo机器人任务、7个Atari游戏；非专家承包商反馈，单条耗时3-5秒；对比传统RL、合成Oracle反馈
- 经典任务结果
  - 机器人任务：700条反馈达传统RL性能，1400条部分任务略超
  - Atari游戏：5500条反馈在BeamRider/Pong等匹配/超越传统RL，复杂游戏实现显著学习
- 新颖行为学习：900条反馈实现Hopper后空翻；800条实现半猎豹单腿前进；1300条实现Enduro跟车行驶
- 消融实验结论
  - 在线反馈关键：离线反馈易导致智能体行为异常
  - 片段对比更优：长片段提供上下文，提升反馈有效性
  - 正则化与集成：提升奖励函数稳定性和泛化能力
  - 随机查询效果差：基于分歧的查询更具信息价值
## **实验细节**
- 环境改造：移除可变长度episode，消除隐性监督；Atari遮挡分数区域，避免奖励推断
- 机器人任务：TRPO优化，2层神经网络奖励预测，1.5秒片段对比，标签率随步数衰减
- Atari任务：A2C优化，卷积网络奖励预测，1.7秒片段对比，预训练奖励预测器，仅保留最新3000条标签
- 承包商指导：提供任务描述+游戏试玩，按键快速反馈，明确好坏行为判定标准
## **研究结论与展望**
- 核心贡献：将人类偏好反馈规模化应用于深度RL，反馈成本降低3个数量级；解决奖励函数设计痛点，为安全AI提供路径
- 关键发现：人类反馈效率略低于合成反馈，但成本可控；部分任务中人类反馈因奖励塑形表现更优
- 未来方向：提升人类反馈效率；扩展至更多现实复杂任务；缩小偏好学习与显式奖励学习的性能差距

<img width="1450" height="978" alt="image" src="https://github.com/user-attachments/assets/0b2b5681-26d3-498a-8fe5-9ac204b3c6a2" />

</details>

## 2.2 A General Language Assistant as a Laboratory for Alignment. 202112. Anthropic.

<details>

 <summary>点击展开/折叠（论文摘要）</summary>

 ## 1. 研究目标
- 打造**通用、与人类价值观对齐**的文本助手
- 对齐标准：**Helpful, Honest, Harmless (HHH)**
- 研究方向：简单对齐方法 + 训练目标缩放规律
## 2. 研究路线（三大块）
1. 轻量级对齐干预（Prompting）
2. 三类训练目标的缩放对比
3. 偏好模型预训练（提升样本效率）
## 3. 第一部分：Prompting 对齐
- 方法：HHH 提示词、示例对话
- 发现：
  - 效果**随模型规模变大而提升**
  - 泛化到多种对齐评估
  - **不牺牲模型性能**（无 alignment tax）
- 对比：
  - Full HHH prompt
  - Short prompt（单轮对话）
  - Context-distilled 版本
## 4. 第二部分：三类训练目标对比（核心）
### 4.1 Imitation Learning（模仿学习）
- 目标：只学**好样本**的生成
- 损失：标准自回归 LM 损失
- 技巧：**mask 只在回答部分算损失**
- 评估：用**平均负对数概率**打分
### 4.2 Binary Discrimination（二元判别）
- 目标：二分类 → 好 / 坏
- 数据：好坏样本对
- 结论：**效果 ≈ 模仿学习**，缩放几乎一样
### 4.3 Preference Modeling（偏好建模）
- 目标：学习**优劣排序**，不只二分类
- 数据：按质量排序的样本对
- 损失：成对排序损失
- 结论：**远优于模仿学习，缩放更优**
## 5. 第三部分：Preference Model Pre-training (PMP)
- 目的：提升**人类偏好微调的样本效率**
- 做法：
  1. 先在大规模偏好数据上预训练 PM
  2. 再用少量人类标注微调
- 效果：小数据下效果显著提升
## 6. 评估方法（acc@k）
- 指标：从 k 个样本中选**最高分**的正确率
- 基于 N ≥ k 个样本做**无偏估计**
- 公式：加权求和 + 组合数概率
## 7. 核心结论
1. 二元判别 ≈ 模仿学习，提升有限
2. **排序偏好建模 >> 模仿学习**
3. 偏好模型对**人类对齐工作非常关键**
4. 轻量提示 + 蒸馏可实现高效对齐

## 8. 如何训练preference model
以预训练大 LM + 价值头为基础架构，以优劣样本对为训练数据，以成对对比损失为核心，通过EOC 结束符、混合语言建模损失做基础优化；为提升样本效率，还可通过大规模公开数据做 PMP 预训练，再用小批量人类标注数据做下游微调，最终实现对人类偏好（二元 / 排序）的精准判断。


</details>

## 2.3 Training a helpful and harmless assistant with reinforcement learning from human feedback. 202204. Anthropic.

<details>
   <summary>点击展开/折叠（论文摘要）</summary>
  
## 研究主题
- 核心目标：通过人类反馈强化学习（RLHF）训练有益且无害的语言模型助手
- 关键技术：偏好建模（PMing）+ RLHF
- 核心发现：对齐训练提升NLP任务性能，兼容专业技能训练
## 数据收集
- 任务说明
  - 帮助性任务：众包工作者与模型对话，选择更有帮助、诚实的响应
  - 无害性任务（红队测试）：众包工作者尝试诱导模型产生有害响应，选择更有害的响应
- 数据集分类
  - 基础数据集：44k帮助性对比 + 42k红队测试对比
  - 拒绝采样（RS）数据集：52k帮助性对比 + 2k红队测试对比
  - 迭代在线数据集：22k帮助性对比（无红队数据）
- 众包工作者
  - 来源：美国MTurk（占80%数据）、Upwork
  - 筛选：基于对话质量而非标签一致性，建立核心工作者团队
## 偏好建模（PM）
- 训练设置
  - 模型规模：13M-52B参数（几何级数递增）
  - 训练阶段：LM预训练 → 偏好模型预训练（PMP） → 人类反馈微调
  - 训练细节：单轮训练，上下文长度1024（在线模型2048）
- 核心结果
  - 缩放趋势：模型/数据集规模与PM准确率呈对数线性关系
  - 校准特性：仅训练帮助性数据的PM校准良好，混合数据集PM略欠自信
  - 评估表现：在HHH评估中准确率86%（超人类平均75%），能区分有害/无害对话
- 关键发现：模型规模越大，对帮助性/无害性数据混合比例的鲁棒性越强
## 人类反馈强化学习（RLHF）
- 训练设置
  - 初始化：基于上下文蒸馏模型
  - 算法：近端策略优化（PPO），KL惩罚系数λ=0.001
  - 提示数据集：137k静态提示 + 369k模型生成提示
- 核心结果
  - 性能影响：大模型（13B/52B）零样本NLP任务性能提升，小模型存在"对齐代价"
  - 鲁棒性：大PM更稳健，RL训练后期存在过拟合（训练PM与测试PM分数偏离）
  - 线性关系：√D_KL（策略与初始策略的KL散度平方根）与奖励近似线性相关
- 关键挑战
  - 帮助性与无害性的张力：过度优化无害性导致响应回避，需平衡两者
  - 在线训练改进：每周迭代更新PM和RL策略，提升数据质量和模型偏好度
## 扩展研究
- 多目标与专业技能
  - 混合目标：PM可同时学习帮助性、无害性与专业技能（摘要、编码），无性能损失
  - 编码任务：对代码微调模型应用自然语言RLHF，提升HumanEval任务表现
- 分布外（OOD）检测
  - 方法：简化相对马氏距离，基于帮助性数据激活向量检测有害请求
  - 效果：52B模型中层激活检测AUROC达0.85，少量异常样本暴露后可提升至0.94
- 偏见与诚实性
  - 诚实性：RLHF提升TruthfulQA表现，模型规模越大效果越显著
  - 偏见：对所有种族/宗教群体的情感更积极，但性别偏见与基础LM强相关
## 定性评估与对比
- 与人类作家对比：众包工作者57%偏好在线HH模型，PM对模型响应评分高于人类写作
- 敏感问题处理：部分回避主观/敏感话题，对有害请求礼貌拒绝并提供相关信息
- 对话示例：支持多轮交互，能遵循指令完成邮件撰写、技术问题解答等任务
## 讨论与局限
- 核心贡献
  - 验证RLHF在对齐与能力提升上的兼容性
  - 提出迭代在线训练范式，解决高分数区间数据稀缺问题
  - 建立帮助性/无害性对齐的技术框架
- 局限
  - 诚实性优化不足，需结合其他技术
  - 小模型存在对齐代价，鲁棒性待提升
  - 数据收集存在偏差，有害性数据缺乏正向引导
- 未来方向
  - 改进无害性数据收集（引导模型积极应对有害请求）
  - 结合检索增强等技术提升诚实性
  - 构建公共对齐数据集，推动安全AI研究
 
## 核心技术框架图
<img width="1718" height="936" alt="image" src="https://github.com/user-attachments/assets/0ade9291-ee37-4c01-9164-12899cef406a" />

    
</details>

## 2.4 Constitutional AI: Harmlessness from AI Feedback. 202212. Anthropic.

<details>
   <summary>点击展开/折叠（论文摘要）</summary>
   
## 研究主题
- 核心目标：通过人类反馈强化学习（RLHF）训练有益且无害的语言模型助手
- 关键技术：偏好建模（PMing）+ RLHF
- 核心发现：对齐训练提升NLP任务性能，兼容专业技能训练
## 数据收集
- 任务说明
  - 帮助性任务：众包工作者与模型对话，选择更有帮助、诚实的响应
  - 无害性任务（红队测试）：众包工作者尝试诱导模型产生有害响应，选择更有害的响应
- 数据集分类
  - 基础数据集：44k帮助性对比 + 42k红队测试对比
  - 拒绝采样（RS）数据集：52k帮助性对比 + 2k红队测试对比
  - 迭代在线数据集：22k帮助性对比（无红队数据）
- 众包工作者
  - 来源：美国MTurk（占80%数据）、Upwork
  - 筛选：基于对话质量而非标签一致性，建立核心工作者团队
## 偏好建模（PM）
- 训练设置
  - 模型规模：13M-52B参数（几何级数递增）
  - 训练阶段：LM预训练 → 偏好模型预训练（PMP） → 人类反馈微调
  - 训练细节：单轮训练，上下文长度1024（在线模型2048）
- 核心结果
  - 缩放趋势：模型/数据集规模与PM准确率呈对数线性关系
  - 校准特性：仅训练帮助性数据的PM校准良好，混合数据集PM略欠自信
  - 评估表现：在HHH评估中准确率86%（超人类平均75%），能区分有害/无害对话
- 关键发现：模型规模越大，对帮助性/无害性数据混合比例的鲁棒性越强
## 人类反馈强化学习（RLHF）
- 训练设置
  - 初始化：基于上下文蒸馏模型
  - 算法：近端策略优化（PPO），KL惩罚系数λ=0.001
  - 提示数据集：137k静态提示 + 369k模型生成提示
- 核心结果
  - 性能影响：大模型（13B/52B）零样本NLP任务性能提升，小模型存在"对齐代价"
  - 鲁棒性：大PM更稳健，RL训练后期存在过拟合（训练PM与测试PM分数偏离）
  - 线性关系：√D_KL（策略与初始策略的KL散度平方根）与奖励近似线性相关
- 关键挑战
  - 帮助性与无害性的张力：过度优化无害性导致响应回避，需平衡两者
  - 在线训练改进：每周迭代更新PM和RL策略，提升数据质量和模型偏好度
## 扩展研究
- 多目标与专业技能
  - 混合目标：PM可同时学习帮助性、无害性与专业技能（摘要、编码），无性能损失
  - 编码任务：对代码微调模型应用自然语言RLHF，提升HumanEval任务表现
- 分布外（OOD）检测
  - 方法：简化相对马氏距离，基于帮助性数据激活向量检测有害请求
  - 效果：52B模型中层激活检测AUROC达0.85，少量异常样本暴露后可提升至0.94
- 偏见与诚实性
  - 诚实性：RLHF提升TruthfulQA表现，模型规模越大效果越显著
  - 偏见：对所有种族/宗教群体的情感更积极，但性别偏见与基础LM强相关
## 定性评估与对比
- 与人类作家对比：众包工作者57%偏好在线HH模型，PM对模型响应评分高于人类写作
- 敏感问题处理：部分回避主观/敏感话题，对有害请求礼貌拒绝并提供相关信息
- 对话示例：支持多轮交互，能遵循指令完成邮件撰写、技术问题解答等任务
## 讨论与局限
- 核心贡献
  - 验证RLHF在对齐与能力提升上的兼容性
  - 提出迭代在线训练范式，解决高分数区间数据稀缺问题
  - 建立帮助性/无害性对齐的技术框架
- 局限
  - 诚实性优化不足，需结合其他技术
  - 小模型存在对齐代价，鲁棒性待提升
  - 数据收集存在偏差，有害性数据缺乏正向引导
- 未来方向
  - 改进无害性数据收集（引导模型积极应对有害请求）
  - 结合检索增强等技术提升诚实性
  - 构建公共对齐数据集，推动安全AI研究

## 技术框架图
<img width="1498" height="862" alt="image" src="https://github.com/user-attachments/assets/9d74e6ea-21ca-4ef9-9282-f9726469bcd3" />

</details>


## 2.5 HIGH-DIMENSIONAL CONTINUOUS CONTROL USING GENERALIZED ADVANTAGE ESTIMATION. ICLR 2016. UCB.

<details>
   <summary>点击展开/折叠（论文摘要）</summary>
    
这篇发表于ICLR 2016的经典论文由伯克利团队提出**广义优势估计（GAE）**，核心解决强化学习中策略梯度方法**样本复杂度高、训练不稳定**的痛点，实现了高维连续控制任务的高效稳定学习，核心内容可总结为以下5点：
1. **研究背景**：策略梯度方法虽能直接优化累积奖励、适配神经网络，但存在两大问题——无偏梯度估计方差随时间范围剧增，需海量样本；演员-评论家方法降方差却引入偏差，易导致算法不收敛，且高维连续控制（如机器人运动）对稳定性和效率要求更高。
2. **核心创新**：提出**GAE(γ,λ)**，通过指数加权平均多步时间差分（TD）残差估计优势函数，以**γ∈[0,1]**（折扣因子）和**λ∈[0,1]**（加权因子）两个独立参数**可控平衡偏差-方差**：γ调控全局不可消除偏差，最优值极接近1；λ调控价值函数近似带来的局部偏差，最优值0.9~0.99，二者结合在可容忍偏差下大幅降低方差。
3. **配套优化**：将**信任区域优化**同时应用于策略更新（TRPO）和价值函数训练，约束策略与价值函数的更新幅度（KL散度阈值），避免过拟合和更新幅度过大导致的训练崩溃，进一步提升稳定性。
4. **实验验证**：在倒立摆、3D双足/四足机器人行走、双足机器人站立等高维连续控制任务中验证效果，采用神经网络直接映射运动学数据到关节扭矩（无需手工设计策略），实证GAE最优参数下，3D双足行走仅需约5.8天真实模拟时间即可学到稳定步态，相比无价值函数基线大幅降低样本需求。
5. **核心贡献与意义**：①为策略梯度提供通用的方差减少方案，GAE适配在线/批处理场景；②提出价值函数的信任区域优化方法，实现高参神经网络的稳定训练；③首次实现端到端的高维连续控制，拓展了强化学习在机器人运动等领域的应用边界，也为后续PPO等经典算法奠定了基础。

整体而言，论文的核心逻辑是**以GAE实现偏差-方差的精细化权衡，结合信任区域优化保障训练稳定**，最终解决了策略梯度方法在高维连续控制中的核心痛点。

## 核心公式
<img width="1400" height="988" alt="image" src="https://github.com/user-attachments/assets/0b15ab7e-0a20-4609-b8d4-7adef7ed92d4" />

</details>

# 3. 数据集

# 4. 推理增强

# 5. 智能体
