# awesome-LLM-papers
经典LLM论文阅读和复现。

## Training a helpful and harmless assistant with reinforcement learning from human feedback. 2022. Anthropic.

<details>
   <summary>点击展开/折叠（论文摘要）</summary>
  
## 研究主题
- 核心目标：通过人类反馈强化学习（RLHF）训练有益且无害的语言模型助手
- 关键技术：偏好建模（PMing）+ RLHF
- 核心发现：对齐训练提升NLP任务性能，兼容专业技能训练
## 数据收集
- 任务说明
  - 帮助性任务：众包工作者与模型对话，选择更有帮助、诚实的响应
  - 无害性任务（红队测试）：众包工作者尝试诱导模型产生有害响应，选择更有害的响应
- 数据集分类
  - 基础数据集：44k帮助性对比 + 42k红队测试对比
  - 拒绝采样（RS）数据集：52k帮助性对比 + 2k红队测试对比
  - 迭代在线数据集：22k帮助性对比（无红队数据）
- 众包工作者
  - 来源：美国MTurk（占80%数据）、Upwork
  - 筛选：基于对话质量而非标签一致性，建立核心工作者团队
## 偏好建模（PM）
- 训练设置
  - 模型规模：13M-52B参数（几何级数递增）
  - 训练阶段：LM预训练 → 偏好模型预训练（PMP） → 人类反馈微调
  - 训练细节：单轮训练，上下文长度1024（在线模型2048）
- 核心结果
  - 缩放趋势：模型/数据集规模与PM准确率呈对数线性关系
  - 校准特性：仅训练帮助性数据的PM校准良好，混合数据集PM略欠自信
  - 评估表现：在HHH评估中准确率86%（超人类平均75%），能区分有害/无害对话
- 关键发现：模型规模越大，对帮助性/无害性数据混合比例的鲁棒性越强
## 人类反馈强化学习（RLHF）
- 训练设置
  - 初始化：基于上下文蒸馏模型
  - 算法：近端策略优化（PPO），KL惩罚系数λ=0.001
  - 提示数据集：137k静态提示 + 369k模型生成提示
- 核心结果
  - 性能影响：大模型（13B/52B）零样本NLP任务性能提升，小模型存在"对齐代价"
  - 鲁棒性：大PM更稳健，RL训练后期存在过拟合（训练PM与测试PM分数偏离）
  - 线性关系：√D_KL（策略与初始策略的KL散度平方根）与奖励近似线性相关
- 关键挑战
  - 帮助性与无害性的张力：过度优化无害性导致响应回避，需平衡两者
  - 在线训练改进：每周迭代更新PM和RL策略，提升数据质量和模型偏好度
## 扩展研究
- 多目标与专业技能
  - 混合目标：PM可同时学习帮助性、无害性与专业技能（摘要、编码），无性能损失
  - 编码任务：对代码微调模型应用自然语言RLHF，提升HumanEval任务表现
- 分布外（OOD）检测
  - 方法：简化相对马氏距离，基于帮助性数据激活向量检测有害请求
  - 效果：52B模型中层激活检测AUROC达0.85，少量异常样本暴露后可提升至0.94
- 偏见与诚实性
  - 诚实性：RLHF提升TruthfulQA表现，模型规模越大效果越显著
  - 偏见：对所有种族/宗教群体的情感更积极，但性别偏见与基础LM强相关
## 定性评估与对比
- 与人类作家对比：众包工作者57%偏好在线HH模型，PM对模型响应评分高于人类写作
- 敏感问题处理：部分回避主观/敏感话题，对有害请求礼貌拒绝并提供相关信息
- 对话示例：支持多轮交互，能遵循指令完成邮件撰写、技术问题解答等任务
## 讨论与局限
- 核心贡献
  - 验证RLHF在对齐与能力提升上的兼容性
  - 提出迭代在线训练范式，解决高分数区间数据稀缺问题
  - 建立帮助性/无害性对齐的技术框架
- 局限
  - 诚实性优化不足，需结合其他技术
  - 小模型存在对齐代价，鲁棒性待提升
  - 数据收集存在偏差，有害性数据缺乏正向引导
- 未来方向
  - 改进无害性数据收集（引导模型积极应对有害请求）
  - 结合检索增强等技术提升诚实性
  - 构建公共对齐数据集，推动安全AI研究
    
</details>
